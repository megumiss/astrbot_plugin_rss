import aiohttp
import asyncio
import time
import re
import logging
import os
from datetime import datetime, timezone
from zoneinfo import ZoneInfo
from lxml import etree
from apscheduler.schedulers.asyncio import AsyncIOScheduler
# from astrbot.core.platform.sources.aiocqhttp.aiocqhttp_message_event import (
#     AiocqhttpMessageEvent,
# )
from astrbot.api.event import filter, AstrMessageEvent, MessageEventResult, MessageChain
from astrbot.api.star import Context, Star, register
from astrbot.api import AstrBotConfig
import astrbot.api.message_components as Comp

from aiocqhttp.exceptions import ActionFailed

from .data_handler import DataHandler
from .pic_handler import RssImageHandler
from .rss import RSSItem
from typing import List, Dict, Tuple, Optional


@register(
    "astrbot_plugin_rss",
    "megumiss",
    "RSSè®¢é˜…æ’ä»¶",
    "1.1.5",
    "https://github.com/megumiss/astrbot_plugin_rss",
)
class RssPlugin(Star):
    def __init__(self, context: Context, config: AstrBotConfig) -> None:
        super().__init__(context)

        self.logger = logging.getLogger("astrbot")
        self.context = context
        self.config = config
        self.data_handler = DataHandler()
        
        # æå–schemeæ–‡ä»¶ä¸­çš„é…ç½®
        self.title_max_length = config.get("title_max_length")
        self.description_max_length = config.get("description_max_length")
        self.max_items_per_poll = config.get("max_items_per_poll")
        self.t2i = config.get("t2i")
        self.is_hide_url = config.get("is_hide_url")
        self.is_compose = config.get("compose")
        self.is_download_video = config.get("is_download_video", False)
        
        # å›¾ç‰‡é…ç½®
        self.is_read_pic = config.get("pic_config").get("is_read_pic")
        self.is_adjust_pic = config.get("pic_config").get("is_adjust_pic")
        self.max_pic_item = config.get("pic_config").get("max_pic_item")
        self.cleanup_cron = config.get("pic_config").get("cleanup_cron")
        self.cleanup_retention = config.get("pic_config").get("cleanup_retention")
        # æ—¶åŒºé…ç½®
        self.time_zone = config.get("time_zone", "Asia/Shanghai")
        
        self.pic_handler = RssImageHandler(self.is_adjust_pic)
        
        # ç¼“å­˜ä¸é”
        self.cache_timeout = config.get("cache_timeout", 60) # ç¼“å­˜æœ‰æ•ˆæœŸ
        self.feed_cache: Dict[str, Dict] = {}  # æ ¼å¼: {url: {'ts': timestamp, 'items': [RSSItem]}}
        self.fetch_locks: Dict[str, asyncio.Lock] = {} # æ ¼å¼: {url: asyncio.Lock()}

        self.scheduler = AsyncIOScheduler()
        self.scheduler.start()

        # æ¸…ç†ä»»åŠ¡
        self._add_cleanup_job()
        self._fresh_asyncIOScheduler()

    def parse_cron_expr(self, cron_expr: str):
        fields = cron_expr.split(" ")
        return {
            "minute": fields[0],
            "hour": fields[1],
            "day": fields[2],
            "month": fields[3],
            "day_of_week": fields[4],
        }

    async def terminate(self):
        """æ’ä»¶å¸è½½/é‡è½½æ—¶çš„æ¸…ç†å·¥ä½œ"""
        self.logger.info("RSSæ’ä»¶æ­£åœ¨å¸è½½ï¼Œå‡†å¤‡åœæ­¢è°ƒåº¦å™¨...")
        try:
            if hasattr(self, 'scheduler') and self.scheduler.running:
                self.scheduler.shutdown()
                self.logger.info("RSSæ’ä»¶è°ƒåº¦å™¨å·²åœæ­¢ã€‚")
        except Exception as e:
            self.logger.error(f"åœæ­¢RSSæ’ä»¶è°ƒåº¦å™¨æ—¶å‘ç”Ÿé”™è¯¯: {e}")

    async def parse_channel_info(self, url):
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
        connector = aiohttp.TCPConnector(ssl=False)
        timeout = aiohttp.ClientTimeout(total=30, connect=10)
        try:
            async with aiohttp.ClientSession(trust_env=True,
                                             connector=connector,
                                             timeout=timeout,
                                             headers=headers
                                             ) as session:
                async with session.get(url) as resp:
                    if resp.status != 200:
                        self.logger.error(f"rss: æ— æ³•æ­£å¸¸æ‰“å¼€ç«™ç‚¹ {url}")
                        return None
                    text = await resp.read()
                    return text
        except asyncio.TimeoutError:
            self.logger.error(f"rss: è¯·æ±‚ç«™ç‚¹ {url} è¶…æ—¶")
            return None
        except aiohttp.ClientError as e:
            self.logger.error(f"rss: è¯·æ±‚ç«™ç‚¹ {url} ç½‘ç»œé”™è¯¯: {str(e)}")
            return None
        except Exception as e:
            self.logger.error(f"rss: è¯·æ±‚ç«™ç‚¹ {url} å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}")
            return None

    async def _safe_send_message(self, user: str, message_chain: MessageChain):
        """
        ç»Ÿä¸€å‘é€æ¶ˆæ¯æ–¹æ³•ï¼ŒåŒ…å«é£æ§é‡è¯•é€»è¾‘
        """
        
        # è¾…åŠ©å‡½æ•°ï¼šæ—‹è½¬å•ä¸ªç»„ä»¶ï¼ˆå¦‚æœå®ƒæ˜¯å›¾ç‰‡ï¼‰
        def _try_rotate_component(component) -> bool:
            if isinstance(component, Comp.Image) and component.file:
                file_path = component.file
                if file_path.startswith("file://"):
                    file_path = file_path.replace("file://", "")
                return self.pic_handler.rotate_image_180(file_path)
            return False

        # è¾…åŠ©å‡½æ•°ï¼šå¤„ç† Node å†…éƒ¨çš„å†…å®¹
        def _process_node_content(node_component) -> bool:
            rotated = False
            node_content = getattr(node_component, "content", [])
            if isinstance(node_content, list):
                for sub_comp in node_content:
                    if _try_rotate_component(sub_comp):
                        rotated = True
            return rotated

        try:
            await self.context.send_message(user, message_chain)
        except ActionFailed as e:
            # æ•è· NTQQ çš„ Timeout (é£æ§) é”™è¯¯
            if e.retcode == 1200:
                self.logger.warning(f"[RSS] å‘é€å¤±è´¥(Retcode 1200/Timeout)ï¼Œç–‘ä¼¼é£æ§ï¼Œå°è¯•æ—‹è½¬å›¾ç‰‡é‡è¯•...")
                
                has_rotated = False
                # éå†æ¶ˆæ¯é“¾ï¼Œæ‰¾åˆ°æ‰€æœ‰å›¾ç‰‡ç»„ä»¶
                for component in message_chain.chain:
                    # 1. ç›´æ¥æ˜¯å›¾ç‰‡
                    if _try_rotate_component(component):
                        has_rotated = True
                    # 2. æ˜¯ Nodes å®¹å™¨ï¼Œåˆå¹¶è½¬å‘
                    elif isinstance(component, Comp.Nodes):
                        # éå†å®¹å™¨å†…çš„æ‰€æœ‰ Node
                        for sub_node in component.nodes:
                            if _process_node_content(sub_node):
                                has_rotated = True
                    # 3. æ˜¯ Node å•ä¸ªèŠ‚ç‚¹ï¼Œå•æ¡è½¬å‘
                    elif isinstance(component, Comp.Node):
                        if _process_node_content(component):
                            has_rotated = True
                
                if has_rotated:
                    try:
                        # ç¨å¾®ç­‰å¾…ä¸€ä¸‹å†é‡è¯•
                        await asyncio.sleep(1)
                        self.logger.info("[RSS] é‡è¯•å‘é€æ—‹è½¬åçš„æ¶ˆæ¯...")
                        # æ·»åŠ ç®€çŸ­æ–‡å­—æç¤º
                        message_chain.chain.append(Comp.Plain("\nå›¾ç‰‡å·²è¢«æ—‹è½¬"))
                        await self.context.send_message(user, message_chain)
                        return # é‡è¯•æˆåŠŸï¼Œé€€å‡º
                    except Exception as retry_e:
                        self.logger.error(f"[RSS] é‡è¯•å‘é€ä¾ç„¶å¤±è´¥: {retry_e}")
            
            # å¦‚æœä¸æ˜¯ 1200 æˆ–è€…é‡è¯•ä¹ŸæŒ‚äº†ï¼Œæ‰“å°é”™è¯¯ä½†ä¸é˜»æ–­æµç¨‹
            self.logger.error(f"[RSS] æ¶ˆæ¯å‘é€æœ€ç»ˆå¤±è´¥: {e}")
        except Exception as e:
            self.logger.error(f"[RSS] å‘é€é‡åˆ°æœªçŸ¥é”™è¯¯: {e}")

    async def cron_task_callback(self, url: str, user: str):
        """å®šæ—¶ä»»åŠ¡å›è°ƒ"""
        if url not in self.data_handler.data:
            return
        if user not in self.data_handler.data[url]["subscribers"]:
            return

        # 1. æ„å»ºæ—¥å¿—å‰ç¼€ [RSS][ç”¨æˆ·][URLæœ«å°¾] ä»¥éš”ç¦»æ—¥å¿—
        clean_url = url.split("//")[-1]
        short_url = "..." + clean_url[-25:] if len(clean_url) > 25 else clean_url
        log_prefix = f"[RSS][{user}][{short_url}]"
        self.logger.info(f"{log_prefix} ä»»åŠ¡è§¦å‘")

        sub_info = self.data_handler.data[url]["subscribers"][user]
        last_update = sub_info["last_update"]
        latest_link = sub_info["latest_link"]
        max_items_per_poll = self.max_items_per_poll
        # æ‹‰å– RSS
        rss_items = await self.poll_rss(
            url,
            num=max_items_per_poll,
            after_timestamp=last_update,
            after_link=latest_link,
        )

        self.logger.info(f"{log_prefix} æ‹‰å–å®Œæˆï¼Œè·å–åˆ° {len(rss_items)} æ¡æ–°å†…å®¹")
        max_ts = last_update

        # å¤„ç†æ¶ˆæ¯å‘é€
        if self.is_compose:
            # åˆå¹¶è½¬å‘æ¨¡å¼
            node_list = []
            for item in rss_items:
                main_comps, video_comp = await self._get_chain_components(item)
                
                # 1. æ–‡æœ¬å’Œå›¾ç‰‡
                node = Comp.Node(
                    uin=0,
                    name="Astrbot",
                    content=main_comps
                )
                node_list.append(node)
                
                # 2. è§†é¢‘å•ç‹¬ä½œä¸ºä¸€ä¸ªNode
                if video_comp:
                    video_node = Comp.Node(
                        uin=0,
                        name="Astrbot",
                        content=[video_comp]
                    )
                    node_list.append(video_node)

                if item.pubDate_timestamp > max_ts:
                    max_ts = item.pubDate_timestamp
            
            if len(node_list) > 0:
                # ä½¿ç”¨ Comp.Nodes å°†åˆ—è¡¨åŒ…è£…æˆä¸€ä¸ªâ€œåˆå¹¶è½¬å‘å®¹å™¨ç»„ä»¶â€
                nodes_container = Comp.Nodes(node_list)
                # æ„é€ æ¶ˆæ¯é“¾ï¼šå¿…é¡»åŒ…å«å®¹å™¨ç»„ä»¶ï¼Œä¸”å…³é—­ t2i
                msc = MessageChain(
                    chain=[nodes_container], 
                    use_t2i_=False 
                )
                self.logger.info(f"{log_prefix} æ­£åœ¨å‘é€åˆå¹¶æ¶ˆæ¯ (åŒ…å« {len(node_list)} æ¡)...")
                # è°ƒç”¨ç»Ÿä¸€å‘é€æ–¹æ³•
                await self._safe_send_message(user, msc)
        else:
            # é€æ¡å‘é€æ¨¡å¼
            for idx, item in enumerate(rss_items):
                main_comps, video_comp = await self._get_chain_components(item)
                
                # å‘é€ä¸»ä½“å†…å®¹
                msc = MessageChain(
                    chain=main_comps,
                    use_t2i_=self.t2i
                )
                await self._safe_send_message(user, msc)
                
                # å¦‚æœæœ‰è§†é¢‘ï¼Œå•ç‹¬å‘é€ä¸€æ¡æ¶ˆæ¯
                if video_comp:
                    video_msc = MessageChain(chain=[video_comp], use_t2i_=False)
                    await self._safe_send_message(user, video_msc)

                self.logger.info(f"{log_prefix} ç¬¬ {idx+1}/{len(rss_items)} æ¡å·²å‘é€")

                # åªè®°å½• item çš„æ—¶é—´æˆ³ï¼Œä¸ä½¿ç”¨ç³»ç»Ÿæ—¶é—´
                if item.pubDate_timestamp > max_ts:
                    max_ts = item.pubDate_timestamp

        # æ›´æ–°æœ€åæ›´æ–°æ—¶é—´
        if rss_items:
            # åªæœ‰å½“ç¡®å®å¤„ç†äº†æ¶ˆæ¯ï¼Œæ‰æ›´æ–°æ•°æ®åº“
            self.data_handler.data[url]["subscribers"][user]["last_update"] = max_ts
            # æ›´æ–°æœ€æ–°é“¾æ¥ä½œä¸ºåŒé‡æ ¡éªŒ
            self.data_handler.data[url]["subscribers"][user]["latest_link"] = rss_items[0].link
            self.data_handler.save_data()
            self.logger.info(f"RSS å®šæ—¶ä»»åŠ¡ {url} æ¨é€æˆåŠŸ - {user}ï¼Œæ›´æ–°æ—¶é—´è‡³: {max_ts}")
        else:
            self.logger.info(f"RSS å®šæ—¶ä»»åŠ¡ {url} æ— æ¶ˆæ¯æ›´æ–° - {user}")

    async def _fetch_and_parse_feed(self, url: str) -> List[RSSItem]:
        """
        [å†…éƒ¨æ–¹æ³•] æ‰§è¡Œå®é™…çš„ç½‘ç»œè¯·æ±‚å¹¶è§£ææ‰€æœ‰ RSS æ¡ç›®
        ä¸æ‰§è¡Œä»»ä½•è¿‡æ»¤ï¼ˆæ—¶é—´æˆ³/æ•°é‡ï¼‰ï¼Œåªè´Ÿè´£è§£æå¹¶è¿”å›å¯¹è±¡åˆ—è¡¨
        """
        text = await self.parse_channel_info(url)
        if text is None:
            self.logger.error(f"rss: æ— æ³•è§£æç«™ç‚¹ {url} çš„RSSä¿¡æ¯")
            return []
        
        try:
            root = etree.fromstring(text)
        except Exception as e:
            self.logger.error(f"rss: XMLè§£æå¤±è´¥ {url}: {str(e)}")
            return []
        
        # æ£€æµ‹æ˜¯RSSè¿˜æ˜¯Atom
        is_atom = root.tag.endswith('feed') or 'atom' in root.tag.lower()
        
        # æ ¹æ®æ ¼å¼é€‰æ‹©itemè·¯å¾„
        if is_atom:
            items = root.xpath("//*[local-name()='entry']")
        else:
            items = root.xpath("//item")

        rss_items_list = []

        # è·å–é¢‘é“æ ‡é¢˜ï¼Œç”¨äºå¡«å…… RSSItem
        chan_title = (
            self.data_handler.data[url]["info"]["title"]
            if url in self.data_handler.data
            else "æœªçŸ¥é¢‘é“"
        )

        for item in items:
            try:
                # æå–æ ‡é¢˜
                if is_atom:
                    title_elem = item.xpath("*[local-name()='title']")
                else:
                    title_elem = item.xpath("title")
                
                title = title_elem[0].text if title_elem and title_elem[0].text else "æ— æ ‡é¢˜"
                if len(title) > self.title_max_length:
                    title = title[: self.title_max_length] + "..."

                # æå–é“¾æ¥
                if is_atom:
                    link_elem = item.xpath("*[local-name()='link']/@href")
                    link = link_elem[0] if link_elem else ""
                else:
                    link_elem = item.xpath("link")
                    link = link_elem[0].text if link_elem and link_elem[0].text else ""
                
                if link and not re.match(r"^https?://", link):
                    link = self.data_handler.get_root_url(url) + link

                # æå–æè¿°/å†…å®¹ - ä¼˜å…ˆä½¿ç”¨å®Œæ•´å†…å®¹
                content = ""
                description = ""
                summary = ""
                
                if is_atom:
                    # Atomæ ¼å¼ - ä½¿ç”¨local-name()
                    content_elem = item.xpath("*[local-name()='content']")
                    summary_elem = item.xpath("*[local-name()='summary']")
                    
                    if content_elem and content_elem[0].text:
                        content = content_elem[0].text
                    if summary_elem and summary_elem[0].text:
                        summary = summary_elem[0].text
                    description = content or summary
                else:
                    # RSSæ ¼å¼
                    desc_elem = item.xpath("description")
                    # å°è¯•è·å–content:encoded(æ›´å®Œæ•´çš„å†…å®¹)
                    content_elem = item.xpath("*[local-name()='encoded']")
                    
                    if content_elem and content_elem[0].text:
                        content = content_elem[0].text
                    if desc_elem and desc_elem[0].text:
                        description = desc_elem[0].text

                # æå–ä½œè€…
                author = ""
                if is_atom:
                    author_elem = item.xpath("*[local-name()='author']/*[local-name()='name']")
                    if author_elem and author_elem[0].text:
                        author = author_elem[0].text
                else:
                    author_elem = item.xpath("author") or item.xpath("*[local-name()='creator']")
                    if author_elem and author_elem[0].text:
                        author = author_elem[0].text
                
                # æå–åˆ†ç±»
                categories = []
                if is_atom:
                    cat_elems = item.xpath("*[local-name()='category']/@term")
                    categories = list(cat_elems)
                else:
                    cat_elems = item.xpath("category")
                    categories = [cat.text for cat in cat_elems if cat.text]
                
                # æå–é™„ä»¶(enclosure)
                enclosure_url = ""
                enclosure_type = ""
                enclosure_elem = item.xpath("enclosure")
                if enclosure_elem:
                    enclosure_url = enclosure_elem[0].get("url", "")
                    enclosure_type = enclosure_elem[0].get("type", "")
                
                # æå–è¯„è®ºé“¾æ¥
                comments_url = ""
                comments_elem = item.xpath("comments")
                if comments_elem and comments_elem[0].text:
                    comments_url = comments_elem[0].text
                
                # æå–GUID
                guid = ""
                if is_atom:
                    guid_elem = item.xpath("*[local-name()='id']")
                    if guid_elem and guid_elem[0].text:
                        guid = guid_elem[0].text
                else:
                    guid_elem = item.xpath("guid")
                    if guid_elem and guid_elem[0].text:
                        guid = guid_elem[0].text
                
                # å¤„ç†å†…å®¹ - ä½¿ç”¨å®Œæ•´å†…å®¹æˆ–æè¿°
                full_content = content or description
                
                # ä½¿ç”¨ extract_media_urls æ¥å…¨é¢æå–å›¾ç‰‡å’Œè§†é¢‘
                media_data = self.data_handler.extract_media_urls(full_content)
                pic_url_list = media_data["images"]
                
                # å¦‚æœåŸç”Ÿæ²¡æœ‰é™„ä»¶ï¼Œä½† HTML ä¸­æå–åˆ°äº†è§†é¢‘ï¼Œåˆ™å°†ç¬¬ä¸€ä¸ªè§†é¢‘ä½œä¸ºé™„ä»¶
                if not enclosure_url and media_data["videos"]:
                    enclosure_url = media_data["videos"][0]
                    enclosure_type = "video/mp4" # å‡è®¾ä¸º mp4ï¼Œåç»­ä¸‹è½½ä¼šæ ¡éªŒ
                
                # æ¸…ç†HTMLå¾—åˆ°çº¯æ–‡æœ¬æè¿°
                clean_description = self.data_handler.strip_html(description or content)
                clean_description = self.data_handler.smart_truncate(clean_description, self.description_max_length)
                
                # ä¿ç•™å®Œæ•´å†…å®¹
                clean_content = self.data_handler.strip_html(content) if content else ""

                # æå–æ—¥æœŸ
                pub_date = ""
                pub_date_timestamp = 0
                
                if is_atom:
                    date_elem = item.xpath("*[local-name()='updated']") or \
                               item.xpath("*[local-name()='published']")
                    if date_elem and date_elem[0].text:
                        pub_date = date_elem[0].text
                else:
                    date_elem = item.xpath("pubDate")
                    if date_elem and date_elem[0].text:
                        pub_date = date_elem[0].text
                
                # è§£ææ—¥æœŸ
                if pub_date:
                    pub_date_timestamp = self._parse_date(pub_date)
                
                # å°†è§£æå¥½çš„å¯¹è±¡åŠ å…¥åˆ—è¡¨
                rss_items_list.append(
                    RSSItem(
                        chan_title=chan_title,
                        title=title,
                        link=link,
                        description=clean_description,
                        pubDate=pub_date,
                        pubDate_timestamp=pub_date_timestamp,
                        pic_urls=pic_url_list,
                        author=author,
                        categories=categories,
                        content=clean_content,
                        summary=summary,
                        enclosure_url=enclosure_url,
                        enclosure_type=enclosure_type,
                        comments_url=comments_url,
                        guid=guid
                    )
                )

            except Exception as e:
                self.logger.error(f"rss: è§£æRssæ¡ç›® {url} å¤±è´¥: {str(e)}")
                continue

        return rss_items_list

    async def _get_feed_data_safe(self, url: str) -> List[RSSItem]:
        """
        è·å– Feed æ•°æ®ï¼Œå¸¦æœ‰ ç¼“å­˜ å’Œ é” æœºåˆ¶
        """
        # 1. ç¡®ä¿æ¯ä¸ª URL éƒ½æœ‰ä¸€ä¸ªå¯¹åº”çš„é”ï¼Œé˜²æ­¢å¹¶å‘è¯·æ±‚
        if url not in self.fetch_locks:
            self.fetch_locks[url] = asyncio.Lock()
        
        # 2. ä¸Šé”
        async with self.fetch_locks[url]:
            current_time = time.time()
            
            # 3. æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
            if url in self.feed_cache:
                cache_data = self.feed_cache[url]
                # å¦‚æœç¼“å­˜æ—¶é—´åœ¨æœ‰æ•ˆæœŸå†…ï¼ˆä¾‹å¦‚60ç§’ï¼‰ï¼Œç›´æ¥è¿”å›
                if current_time - cache_data['ts'] < self.cache_timeout:
                    # self.logger.debug(f"[RSS] Hit cache for {url}")
                    return cache_data['items']
            
            # 4. ç¼“å­˜å¤±æ•ˆæˆ–ä¸å­˜åœ¨ï¼Œæ‰§è¡Œç½‘ç»œè¯·æ±‚
            items = await self._fetch_and_parse_feed(url)
            
            # 5. æ›´æ–°ç¼“å­˜
            self.feed_cache[url] = {
                'ts': current_time,
                'items': items
            }
            return items

    async def poll_rss(
        self,
        url: str,
        num: int = -1,
        after_timestamp: int = 0,
        after_link: str = "",
    ) -> List[RSSItem]:
        """
        ä»ç«™ç‚¹æ‹‰å–RSSä¿¡æ¯ (ä¼˜åŒ–ç‰ˆ)
        å…ˆä»ç¼“å­˜/ç½‘ç»œè·å–å…¨é‡æ•°æ®ï¼Œå†æ ¹æ® timestamp è¿›è¡Œè¿‡æ»¤
        """
        # è·å–å…¨é‡æ¡ç›®ï¼ˆå¸¦ç¼“å­˜ï¼‰
        all_items = await self._get_feed_data_safe(url)
        
        filtered_items = []
        cnt = 0
        
        # éå†å…¨é‡æ¡ç›®è¿›è¡Œè¿‡æ»¤
        for item in all_items:
            is_new = False
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºæ–°å†…å®¹
            if item.pubDate_timestamp > 0:
                # æ—¶é—´æˆ³å¿…é¡»ä¸¥æ ¼å¤§äºä¸Šæ¬¡æ›´æ–°æ—¶é—´
                is_new = item.pubDate_timestamp > after_timestamp
            else:
                # æ— æ—¶é—´æˆ³é€€åŒ–ä¸ºé“¾æ¥åˆ¤æ–­
                is_new = item.link != after_link
            
            if is_new:
                filtered_items.append(item)
                cnt += 1
                if num != -1 and cnt >= num:
                    break
            elif item.pubDate_timestamp > 0:
                # å‡è®¾ RSS æ˜¯æŒ‰æ—¶é—´å€’åºæ’åˆ—çš„ï¼Œä¸€æ—¦é‡åˆ°æ—§æ¶ˆæ¯ï¼Œåé¢çš„è‚¯å®šæ›´æ—§
                # æ³¨æ„ï¼šå¦‚æœ RSS ä¹±åºï¼Œè¿™é‡Œå¯èƒ½éœ€è¦è°ƒæ•´ï¼Œä½†æ ‡å‡† RSS é»˜è®¤æ˜¯æœ‰åºçš„
                if item.pubDate_timestamp <= after_timestamp:
                    break
        
        return filtered_items

    def _parse_date(self, date_str: str) -> int:
        """è§£æå„ç§æ—¥æœŸæ ¼å¼ä¸ºæ—¶é—´æˆ³"""
        if not date_str:
            return 0
        
        # å¸¸è§æ—¥æœŸæ ¼å¼
        date_formats = [
            "%a, %d %b %Y %H:%M:%S %z",      # RSSæ ‡å‡†: Wed, 02 Oct 2002 13:00:00 GMT
            "%a, %d %b %Y %H:%M:%S GMT",     # RSS GMTæ ¼å¼
            "%Y-%m-%dT%H:%M:%S%z",           # ISO 8601: 2002-10-02T13:00:00+00:00
            "%Y-%m-%dT%H:%M:%SZ",            # ISO 8601 UTC: 2002-10-02T13:00:00Z
            "%Y-%m-%dT%H:%M:%S.%f%z",        # ISO 8601å¸¦æ¯«ç§’
            "%Y-%m-%dT%H:%M:%S.%fZ",         # ISO 8601 UTCå¸¦æ¯«ç§’
            "%Y-%m-%d %H:%M:%S",             # ç®€å•æ ¼å¼
            "%Y/%m/%d %H:%M:%S",             # æ–œæ åˆ†éš”
        ]
        
        # é¢„å¤„ç†
        date_str = date_str.strip()
        if "GMT" in date_str:
            date_str = date_str.replace("GMT", "+0000")
            
        current_ts = int(time.time())
        for fmt in date_formats:
            try:
                dt = datetime.strptime(date_str, fmt)
                if dt.tzinfo is None:
                    dt = dt.replace(tzinfo=timezone.utc)
                ts = int(dt.timestamp())
                
                # æœªæ¥æ—¶é—´é˜²æŠ¤ï¼šè¶…è¿‡1å°æ—¶çš„æœªæ¥æ—¶é—´è§†ä¸ºå¼‚å¸¸ï¼Œä¿®æ­£ä¸ºå½“å‰æ—¶é—´
                if ts > current_ts + 3600:
                     return current_ts
                return ts
            except ValueError:
                continue
        
        return current_ts
    
    def parse_rss_url(self, url: str) -> str:
        """è§£æRSS URLï¼Œç¡®ä¿ä»¥httpæˆ–httpså¼€å¤´"""
        if not re.match(r"^https?://", url):
            if not url.startswith("/"):
                url = "/" + url
            url = "https://" + url
        return url

    def _fresh_asyncIOScheduler(self):
        """åˆ·æ–°å®šæ—¶ä»»åŠ¡"""
        self.logger.info("åˆ·æ–°å®šæ—¶ä»»åŠ¡")
        
        # 1. åˆå§‹åŒ–ç™½åå•ï¼Œé»˜è®¤åŒ…å«ç³»ç»Ÿçº§æ¸…ç†ä»»åŠ¡ID
        active_job_ids = {"rss_image_cleanup"}
        
        # 2. æ”¶é›†æ‰€æœ‰æ´»è·ƒçš„è®¢é˜…ä»»åŠ¡ID
        for url, info in self.data_handler.data.items():
            if url in ["rsshub_endpoints", "settings"]:
                continue
            
            for user, sub_info in info["subscribers"].items():
                # æ„é€ å”¯ä¸€ IDï¼šURL + User
                job_id = f"{url}|{user}"
                active_job_ids.add(job_id)
                
                try:
                    # æ·»åŠ æˆ–æ›´æ–°ä»»åŠ¡
                    # id: æŒ‡å®šå›ºå®šID
                    # replace_existing: å¦‚æœä»»åŠ¡å·²å­˜åœ¨ï¼Œåˆ™æ›´æ–°è§¦å‘å‚æ•°
                    self.scheduler.add_job(
                        self.cron_task_callback,
                        "cron",
                        **self.parse_cron_expr(sub_info["cron_expr"]),
                        args=[url, user],
                        id=job_id,
                        replace_existing=True
                    )
                except Exception as e:
                    self.logger.error(f"æ·»åŠ å®šæ—¶ä»»åŠ¡å¤±è´¥ {job_id}: {str(e)}")

        # 3. æ¸…ç†å·²ç»ä¸å†é…ç½®ä¸­çš„åºŸå¼ƒä»»åŠ¡
        # è·å–è°ƒåº¦å™¨ä¸­å½“å‰æ‰€æœ‰çš„ä»»åŠ¡
        current_jobs = self.scheduler.get_jobs()
        for job in current_jobs:
            # å¦‚æœè°ƒåº¦å™¨é‡Œçš„ä»»åŠ¡IDä¸åœ¨æˆ‘ä»¬éœ€è¦æ´»è·ƒçš„åˆ—è¡¨ä¸­ï¼Œè¯´æ˜è¯¥è®¢é˜…å·²è¢«åˆ é™¤
            if job.id not in active_job_ids:
                try:
                    self.scheduler.remove_job(job.id)
                    self.logger.info(f"æ¸…ç†åºŸå¼ƒä»»åŠ¡: {job.id}")
                except Exception as e:
                    self.logger.error(f"æ¸…ç†åºŸå¼ƒä»»åŠ¡å¤±è´¥ {job.id}: {str(e)}")

        self.logger.info(f"å®šæ—¶ä»»åŠ¡åˆ·æ–°å®Œæˆï¼Œå½“å‰è¿è¡Œä»»åŠ¡æ•°: {len(self.scheduler.get_jobs())}")

    def _add_cleanup_job(self):
        """æ·»åŠ æ¸…ç†ä¸´æ—¶æ–‡ä»¶çš„å®šæ—¶ä»»åŠ¡"""
        try:
            # è§£æ Cron è¡¨è¾¾å¼
            cron_args = self.parse_cron_expr(self.cleanup_cron)
            retention_seconds = self.cleanup_retention * 60
            self.logger.info(f"[RSS] æ³¨å†Œå›¾ç‰‡/è§†é¢‘æ¸…ç†ä»»åŠ¡: Cron[{self.cleanup_cron}] ä¿ç•™æ—¶é•¿[{self.cleanup_retention}åˆ†é’Ÿ]")
            
            self.scheduler.add_job(
                self.pic_handler.cleanup_temp_files,
                "cron",
                **cron_args,
                args=[retention_seconds],
                id="rss_image_cleanup",
                replace_existing=True
            )
        except Exception as e:
            self.logger.error(f"[RSS] æ³¨å†Œå›¾ç‰‡æ¸…ç†ä»»åŠ¡å¤±è´¥: {e}")
            retention_seconds = self.cleanup_retention * 60
            self.scheduler.add_job(
                self.pic_handler.cleanup_temp_files,
                "interval",
                minutes=30,
                args=[retention_seconds],
                id="rss_image_cleanup",
                replace_existing=True
            )

    async def _add_url(self, url: str, cron_expr: str, message: AstrMessageEvent):
        """å†…éƒ¨æ–¹æ³•:æ·»åŠ URLè®¢é˜…çš„å…±ç”¨é€»è¾‘"""
        user = message.unified_msg_origin
        if url in self.data_handler.data:
            latest_item = await self.poll_rss(url)
            if not latest_item:
                return message.plain_result(f"æ— æ³•è·å–RSSå†…å®¹,è¯·æ£€æŸ¥URLæ˜¯å¦æ­£ç¡®")
            self.data_handler.data[url]["subscribers"][user] = {
                "cron_expr": cron_expr,
                "last_update": latest_item[0].pubDate_timestamp,
                "latest_link": latest_item[0].link,
            }
        else:
            try:
                text = await self.parse_channel_info(url)
                if text is None:
                    return message.plain_result(f"æ— æ³•è®¿é—®è¯¥RSSæº,è¯·æ£€æŸ¥URLæ˜¯å¦æ­£ç¡®")
                title, desc = self.data_handler.parse_channel_text_info(text)
                latest_item = await self.poll_rss(url)
                if not latest_item:
                    return message.plain_result(f"RSSæºæ— å¯ç”¨å†…å®¹,è¯·æ£€æŸ¥URLæ˜¯å¦æ­£ç¡®")
            except Exception as e:
                return message.plain_result(f"è§£æé¢‘é“ä¿¡æ¯å¤±è´¥: {str(e)}")

            self.data_handler.data[url] = {
                "subscribers": {
                    user: {
                        "cron_expr": cron_expr,
                        "last_update": latest_item[0].pubDate_timestamp,
                        "latest_link": latest_item[0].link,
                    }
                },
                "info": {
                    "title": title,
                    "description": desc,
                },
            }
        self.data_handler.save_data()
        return self.data_handler.data[url]["info"]

    async def _get_chain_components(self, item: RSSItem) -> Tuple[List[any], Optional[any]]:
        """
        ç»„è£…æ¶ˆæ¯é“¾
        Returns:
            Tuple[List[Component], Optional[Component]]: (ä¸»ä½“æ¶ˆæ¯ç»„ä»¶åˆ—è¡¨, è§†é¢‘ç»„ä»¶(è‹¥æœ‰))
        """
        comps = []
        # æ”¶é›†æ‰€æœ‰çš„æ–‡æœ¬è¡Œ
        text_lines = []
        video_comp = None
        
        # æ ‡é¢˜å’Œé¢‘é“ä¿¡æ¯
        text_lines.append(f"ğŸ“° {item.chan_title}")
        text_lines.append("â”€" * 30)
        text_lines.append(f"ğŸ“Œ {item.title}")
        
        # æ·»åŠ ä½œè€…å’Œåˆ†ç±»
        meta_info = []
        if item.author:
            meta_info.append(f"ğŸ‘¤ {item.author}")
        if item.categories:
              # æœ€å¤šæ˜¾ç¤º3ä¸ªåˆ†ç±»
            meta_info.append(f"ğŸ·ï¸ {', '.join(item.categories[:3])}")
        if item.pubDate and item.pubDate_timestamp > 0:
            # æ ¼å¼åŒ–æ—¥æœŸæ˜¾ç¤º
            try:
                target_tz = ZoneInfo(self.time_zone)
                dt = datetime.fromtimestamp(item.pubDate_timestamp, target_tz)
                meta_info.append(f"ğŸ•’ {dt.strftime('%Y-%m-%d %H:%M')}")
            except Exception as e:
                self.logger.warning(f"[RSS] æ—¶é—´æ ¼å¼åŒ–å¤±è´¥: {e}")
                dt = datetime.fromtimestamp(item.pubDate_timestamp)
                meta_info.append(f"ğŸ•’ {dt.strftime('%Y-%m-%d %H:%M')}")
        
        if meta_info:
            text_lines.append(" | ".join(meta_info))
        
        text_lines.append("â”€" * 30)
        
        # å†…å®¹ - ä½¿ç”¨å®Œæ•´å†…å®¹æˆ–æè¿°
        content_text = item.get_display_content(self.description_max_length)
        if content_text:
            # ç¡®ä¿å†…å®¹æœ¬èº«å‰åä¸å¸¦å¤šä½™ç©ºè¡Œ
            text_lines.append(content_text.strip())
        
        # é“¾æ¥
        if not self.is_hide_url and item.link:
            # æ·»åŠ ä¸€ä¸ªç©ºè¡Œåšåˆ†éš”
            text_lines.append("") 
            text_lines.append(f"ğŸ”— {item.link}")
        
        # é™„ä»¶ä¿¡æ¯(éŸ³é¢‘/è§†é¢‘)
        if item.enclosure_url:
            text_lines.append("") # ç©ºè¡Œåˆ†éš”
            enclosure_info = "ğŸ“ é™„ä»¶: "
            is_video = False
            
            if "audio" in item.enclosure_type:
                enclosure_info += "ğŸµ éŸ³é¢‘ - "
            elif "video" in item.enclosure_type:
                enclosure_info += "ğŸ¬ è§†é¢‘ - "
                is_video = True
            else:
                enclosure_info += "ğŸ“„ æ–‡ä»¶ - "
            enclosure_info += item.enclosure_url
            text_lines.append(enclosure_info)

            # è§†é¢‘ç»„ä»¶å¤„ç†
            if is_video:
                if self.is_download_video:
                    file_path = await self.pic_handler.get_video_file(item.enclosure_url)
                    if file_path:
                        video_comp = Comp.Video.fromFileSystem(path=file_path)
                    else:
                        text_lines.append("[âŒ] è§†é¢‘ä¸‹è½½å¤±è´¥")
                else:
                    video_comp = Comp.Video.fromURL(url=item.enclosure_url)
            
        # è¯„è®ºé“¾æ¥
        if item.comments_url:
            text_lines.append(f"ğŸ’¬ è¯„è®º: {item.comments_url}")

        # å›¾ç‰‡æ ‡é¢˜
        has_images = self.is_read_pic and item.pic_urls
        if has_images:
            text_lines.append("") # ç©ºè¡Œåˆ†éš”
            text_lines.append(f"ğŸ“· å›¾ç‰‡ ({len(item.pic_urls)}å¼ ):")

        # ç”Ÿæˆæ–‡æœ¬
        final_text = "\n".join(text_lines)
        comps.append(Comp.Plain(final_text))

        # å¤„ç†å›¾ç‰‡ç»„ä»¶
        if has_images:
            # å¦‚æœmax_pic_itemä¸º-1åˆ™ä¸é™åˆ¶å›¾ç‰‡æ•°é‡
            temp_max_pic_item = len(item.pic_urls) if self.max_pic_item == -1 else self.max_pic_item
            
            for idx, pic_url in enumerate(item.pic_urls[:temp_max_pic_item], 1):
                # è·å–æœ¬åœ°è·¯å¾„
                file_path = await self.pic_handler.get_image_file(pic_url)
                
                if file_path:
                    # ä½¿ç”¨ fromFileSystem å‘é€æœ¬åœ°æ–‡ä»¶
                    comps.append(Comp.Image.fromFileSystem(file_path))
                else:
                    # å›¾ç‰‡åŠ è½½å¤±è´¥çš„ä¿¡æ¯
                    comps.append(Comp.Plain(f"\n[âŒ] å›¾{idx} åŠ è½½å¤±è´¥\n"))
            
            # å¦‚æœè¿˜æœ‰æ›´å¤šå›¾ç‰‡æœªæ˜¾ç¤º
            if len(item.pic_urls) > temp_max_pic_item:
                count = len(item.pic_urls) - temp_max_pic_item
                comps.append(Comp.Plain(f"\n... è¿˜æœ‰ {count} å¼ å›¾ç‰‡æœªæ˜¾ç¤º"))
        
        return comps, video_comp


    def _is_url_or_ip(self,text: str) -> bool:
        """
        åˆ¤æ–­ä¸€ä¸ªå­—ç¬¦ä¸²æ˜¯å¦ä¸ºç½‘å€ï¼ˆhttp/https å¼€å¤´ï¼‰æˆ– IP åœ°å€ã€‚
        """
        url_pattern = r"^(?:http|https)://.+$"
        ip_pattern = r"^((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$"
        return bool(re.match(url_pattern, text) or re.match(ip_pattern, text))

    @filter.command_group("rss", alias={"RSS"})
    def rss(self):
        """RSSè®¢é˜…æ’ä»¶

        å¯ä»¥è®¢é˜…å’Œç®¡ç†å¤šä¸ªRSSæºï¼Œæ”¯æŒcronè¡¨è¾¾å¼è®¾ç½®æ›´æ–°é¢‘ç‡

        cron è¡¨è¾¾å¼æ ¼å¼ï¼š
        * * * * *ï¼Œåˆ†åˆ«è¡¨ç¤ºåˆ†é’Ÿ å°æ—¶ æ—¥ æœˆ æ˜ŸæœŸï¼Œ* è¡¨ç¤ºä»»æ„å€¼ï¼Œæ”¯æŒèŒƒå›´å’Œé€—å·åˆ†éš”ã€‚ä¾‹ï¼š
        1. 0 0 * * * è¡¨ç¤ºæ¯å¤© 0 ç‚¹è§¦å‘ã€‚
        2. 0/5 * * * * è¡¨ç¤ºæ¯ 5 åˆ†é’Ÿè§¦å‘ã€‚
        3. 0 9-18 * * * è¡¨ç¤ºæ¯å¤© 9 ç‚¹åˆ° 18 ç‚¹è§¦å‘ã€‚
        4. 0 0 1,15 * * è¡¨ç¤ºæ¯æœˆ 1 å·å’Œ 15 å· 0 ç‚¹è§¦å‘ã€‚
        æ˜ŸæœŸçš„å–å€¼èŒƒå›´æ˜¯ 0-6ï¼Œ0 è¡¨ç¤ºæ˜ŸæœŸå¤©ã€‚
        """
        pass

    @rss.group("rsshub")
    def rsshub(self, event: AstrMessageEvent):
        """RSSHubç›¸å…³æ“ä½œ

        å¯ä»¥æ·»åŠ ã€æŸ¥çœ‹ã€åˆ é™¤RSSHubçš„ç«¯ç‚¹
        """
        pass

    @rsshub.command("add")
    async def rsshub_add(self, event: AstrMessageEvent, url: str):
        """æ·»åŠ ä¸€ä¸ªRSSHubç«¯ç‚¹

        Args:
            url: RSSHubæœåŠ¡å™¨åœ°å€ï¼Œä¾‹å¦‚ï¼šhttps://rsshub.app
        """
        if url.endswith("/"):
            url = url[:-1]
        # æ£€æŸ¥æ˜¯å¦ä¸ºurlæˆ–ip
        if not self._is_url_or_ip(url):
            yield event.plain_result("è¯·è¾“å…¥æ­£ç¡®çš„URL")
            return
        # æ£€æŸ¥è¯¥ç½‘å€æ˜¯å¦å·²å­˜åœ¨
        elif url in self.data_handler.data["rsshub_endpoints"]:
            yield event.plain_result("è¯¥RSSHubç«¯ç‚¹å·²å­˜åœ¨")
            return
        else:
            self.data_handler.data["rsshub_endpoints"].append(url)
            self.data_handler.save_data()
            yield event.plain_result("æ·»åŠ æˆåŠŸ")

    @rsshub.command("list")
    async def rsshub_list(self, event: AstrMessageEvent):
        """åˆ—å‡ºæ‰€æœ‰å·²æ·»åŠ çš„RSSHubç«¯ç‚¹"""
        ret = "å½“å‰Botæ·»åŠ çš„rsshub endpointï¼š\n"
        yield event.plain_result(
            ret
            + "\n".join(
                [
                    f"{i}: {x}"
                    for i, x in enumerate(self.data_handler.data["rsshub_endpoints"])
                ]
            )
        )

    @rsshub.command("remove")
    async def rsshub_remove(self, event: AstrMessageEvent, idx: int):
        """åˆ é™¤ä¸€ä¸ªRSSHubç«¯ç‚¹

        Args:
            idx: è¦åˆ é™¤çš„ç«¯ç‚¹ç´¢å¼•ï¼Œå¯é€šè¿‡listå‘½ä»¤æŸ¥çœ‹
        """
        if idx < 0 or idx >= len(self.data_handler.data["rsshub_endpoints"]):
            yield event.plain_result("ç´¢å¼•è¶Šç•Œ")
            return
        else:
            # TODO:åˆ é™¤å¯¹åº”çš„å®šæ—¶ä»»åŠ¡
            self.scheduler.remove_job()
            self.data_handler.data["rsshub_endpoints"].pop(idx)
            self.data_handler.save_data()
            yield event.plain_result("åˆ é™¤æˆåŠŸ")

    @rss.command("add")
    async def add_command(
        self,
        event: AstrMessageEvent,
        idx: int,
        route: str,
        minute: str,
        hour: str,
        day: str,
        month: str,
        day_of_week: str,
    ):
        """é€šè¿‡RSSHubè·¯ç”±æ·»åŠ è®¢é˜…

        Args:
            idx: RSSHubç«¯ç‚¹ç´¢å¼•ï¼Œå¯é€šè¿‡/rss rsshub listæŸ¥çœ‹
            route: RSSHubè·¯ç”±ï¼Œéœ€ä»¥/å¼€å¤´
            minute: Cronè¡¨è¾¾å¼åˆ†é’Ÿå­—æ®µ
            hour: Cronè¡¨è¾¾å¼å°æ—¶å­—æ®µ
            day: Cronè¡¨è¾¾å¼æ—¥æœŸå­—æ®µ
            month: Cronè¡¨è¾¾å¼æœˆä»½å­—æ®µ
            day_of_week: Cronè¡¨è¾¾å¼æ˜ŸæœŸå­—æ®µ
        """
        if idx < 0 or idx >= len(self.data_handler.data["rsshub_endpoints"]):
            yield event.plain_result(
                "ç´¢å¼•è¶Šç•Œ, è¯·ä½¿ç”¨ /rss rsshub list æŸ¥çœ‹å·²ç»æ·»åŠ çš„ rsshub endpoint"
            )
            return
        if not route.startswith("/"):
            yield event.plain_result("è·¯ç”±å¿…é¡»ä»¥ / å¼€å¤´")
            return

        url = self.data_handler.data["rsshub_endpoints"][idx] + route
        cron_expr = f"{minute} {hour} {day} {month} {day_of_week}"

        ret = await self._add_url(url, cron_expr, event)
        if isinstance(ret, MessageEventResult):
            yield ret
            return
        else:
            chan_title = ret["title"]
            chan_desc = ret["description"]

        # åˆ·æ–°å®šæ—¶ä»»åŠ¡
        self._fresh_asyncIOScheduler()

        yield event.plain_result(
            f"æ·»åŠ æˆåŠŸã€‚é¢‘é“ä¿¡æ¯ï¼š\næ ‡é¢˜: {chan_title}\næè¿°: {chan_desc}"
        )

    @rss.command("add-url")
    async def add_url_command(
        self,
        event: AstrMessageEvent,
        url: str,
        minute: str,
        hour: str,
        day: str,
        month: str,
        day_of_week: str,
    ):
        """ç›´æ¥é€šè¿‡Feed URLæ·»åŠ è®¢é˜…

        Args:
            url: RSS Feedçš„å®Œæ•´URL
            minute: Cronè¡¨è¾¾å¼åˆ†é’Ÿå­—æ®µ
            hour: Cronè¡¨è¾¾å¼å°æ—¶å­—æ®µ
            day: Cronè¡¨è¾¾å¼æ—¥æœŸå­—æ®µ
            month: Cronè¡¨è¾¾å¼æœˆä»½å­—æ®µ
            day_of_week: Cronè¡¨è¾¾å¼æ˜ŸæœŸå­—æ®µ
        """
        cron_expr = f"{minute} {hour} {day} {month} {day_of_week}"
        ret = await self._add_url(url, cron_expr, event)
        if isinstance(ret, MessageEventResult):
            yield ret
            return
        else:
            chan_title = ret["title"]
            chan_desc = ret["description"]

        # åˆ·æ–°å®šæ—¶ä»»åŠ¡
        self._fresh_asyncIOScheduler()

        yield event.plain_result(
            f"æ·»åŠ æˆåŠŸã€‚é¢‘é“ä¿¡æ¯ï¼š\næ ‡é¢˜: {chan_title}\næè¿°: {chan_desc}"
        )

    @rss.command("list")
    async def list_command(self, event: AstrMessageEvent):
        """åˆ—å‡ºå½“å‰æ‰€æœ‰è®¢é˜…çš„RSSé¢‘é“"""
        user = event.unified_msg_origin
        ret = "å½“å‰è®¢é˜…çš„é¢‘é“ï¼š\n"
        subs_urls = self.data_handler.get_subs_channel_url(user)
        cnt = 0
        for url in subs_urls:
            info = self.data_handler.data[url]["info"]
            ret += f"{cnt}. {info['title']} - {info['description']}\n"
            cnt += 1
        yield event.plain_result(ret)

    @rss.command("remove")
    async def remove_command(self, event: AstrMessageEvent, idx: int):
        """åˆ é™¤ä¸€ä¸ªRSSè®¢é˜…

        Args:
            idx: è¦åˆ é™¤çš„è®¢é˜…ç´¢å¼•ï¼Œå¯é€šè¿‡/rss listæŸ¥çœ‹
        """
        subs_urls = self.data_handler.get_subs_channel_url(event.unified_msg_origin)
        if idx < 0 or idx >= len(subs_urls):
            yield event.plain_result("ç´¢å¼•è¶Šç•Œ, è¯·ä½¿ç”¨ /rss list æŸ¥çœ‹å·²ç»æ·»åŠ çš„è®¢é˜…")
            return
        url = subs_urls[idx]
        self.data_handler.data[url]["subscribers"].pop(event.unified_msg_origin)

        self.data_handler.save_data()

        # åˆ·æ–°å®šæ—¶ä»»åŠ¡
        self._fresh_asyncIOScheduler()
        yield event.plain_result("åˆ é™¤æˆåŠŸ")

    @rss.command("get")
    async def get_command(self, event: AstrMessageEvent, idx: int):
        """è·å–æŒ‡å®šè®¢é˜…çš„æœ€æ–°å†…å®¹

        Args:
            idx: è¦æŸ¥çœ‹çš„è®¢é˜…ç´¢å¼•ï¼Œå¯é€šè¿‡/rss listæŸ¥çœ‹
        """
        subs_urls = self.data_handler.get_subs_channel_url(event.unified_msg_origin)
        if idx < 0 or idx >= len(subs_urls):
            yield event.plain_result("ç´¢å¼•è¶Šç•Œ, è¯·ä½¿ç”¨ /rss list æŸ¥çœ‹å·²ç»æ·»åŠ çš„è®¢é˜…")
            return
        url = subs_urls[idx]
        rss_items = await self.poll_rss(url)
        if not rss_items:
            yield event.plain_result("æ²¡æœ‰æ–°çš„è®¢é˜…å†…å®¹")
            return
        item = rss_items[0]
        # åˆ†è§£MessageSesion
        platform_name, message_type, session_id = event.unified_msg_origin.split(":")
        
        # æ„é€ è¿”å›æ¶ˆæ¯é“¾
        main_comps, video_comp = await self._get_chain_components(item)
        
        # åŒºåˆ†å¹³å°æ„é€ æ¶ˆæ¯é“¾
        if self.is_compose:
            # 1. æ–‡æœ¬å’Œå›¾ç‰‡ Node
            node = Comp.Node(
                    uin=0,
                    name="Astrbot",
                    content=main_comps
                )
            nodes_list = [node]

            # 2. è§†é¢‘ Node
            if video_comp:
                video_node = Comp.Node(
                    uin=0,
                    name="Astrbot",
                    content=[video_comp]
                )
                nodes_list.append(video_node)
            
            # å‘é€åˆå¹¶æ¶ˆæ¯
            nodes_container = Comp.Nodes(nodes_list)
            target_message_chain = MessageChain(chain=[nodes_container], use_t2i_=False)
            await self._safe_send_message(event.unified_msg_origin, target_message_chain)
        else:
            # å•æ¡å‘é€æ¨¡å¼
            target_message_chain = MessageChain(chain=main_comps, use_t2i_=self.t2i)
            await self._safe_send_message(event.unified_msg_origin, target_message_chain)
            
            # è§†é¢‘ç‹¬ç«‹å‘é€
            if video_comp:
                video_chain = MessageChain(chain=[video_comp], use_t2i_=False)
                await self._safe_send_message(event.unified_msg_origin, video_chain)