import aiohttp
import asyncio
import time
import re
import logging
import os
from lxml import etree
from apscheduler.schedulers.asyncio import AsyncIOScheduler

from astrbot.api.event import filter, AstrMessageEvent, MessageEventResult,MessageChain
from astrbot.api.star import Context, Star, register
from astrbot.api import AstrBotConfig
import astrbot.api.message_components as Comp

from .data_handler import DataHandler
from .pic_handler import RssImageHandler
from .rss import RSSItem
from typing import List


@register(
    "astrbot_plugin_rss",
    "megumiss",
    "RSSè®¢é˜…æ’ä»¶",
    "1.0.4",
    "https://github.com/megumiss/astrbot_plugin_rss",
)
class RssPlugin(Star):
    def __init__(self, context: Context, config: AstrBotConfig) -> None:
        super().__init__(context)

        self.logger = logging.getLogger("astrbot")
        self.context = context
        self.config = config
        self.data_handler = DataHandler()
        self.pic_handler = RssImageHandler()

        # æå–schemeæ–‡ä»¶ä¸­çš„é…ç½®
        self.title_max_length = config.get("title_max_length")
        self.description_max_length = config.get("description_max_length")
        self.max_items_per_poll = config.get("max_items_per_poll")
        self.t2i = config.get("t2i")
        self.is_hide_url = config.get("is_hide_url")
        self.is_read_pic= config.get("pic_config").get("is_read_pic")
        self.is_adjust_pic= config.get("pic_config").get("is_adjust_pic")
        self.max_pic_item = config.get("pic_config").get("max_pic_item")
        self.is_compose = config.get("compose")

        self.pic_handler = RssImageHandler(self.is_adjust_pic)
        self.scheduler = AsyncIOScheduler()
        self.scheduler.start()

        # å®šæœŸæ¸…ç†ä¸´æ—¶æ–‡ä»¶çš„ä»»åŠ¡
        self.scheduler.add_job(
            self.pic_handler.cleanup_temp_files, 
            "interval",
            minutes=30,
            id="rss_image_cleanup",
            replace_existing=True
        )

        self._fresh_asyncIOScheduler()

    def parse_cron_expr(self, cron_expr: str):
        fields = cron_expr.split(" ")
        return {
            "minute": fields[0],
            "hour": fields[1],
            "day": fields[2],
            "month": fields[3],
            "day_of_week": fields[4],
        }

    async def parse_channel_info(self, url):
        headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
        connector = aiohttp.TCPConnector(ssl=False)
        timeout = aiohttp.ClientTimeout(total=30, connect=10)
        try:
            async with aiohttp.ClientSession(trust_env=True,
                                        connector=connector,
                                        timeout=timeout,
                                        headers=headers
                                        ) as session:
                async with session.get(url) as resp:
                    if resp.status != 200:
                        self.logger.error(f"rss: æ— æ³•æ­£å¸¸æ‰“å¼€ç«™ç‚¹ {url}")
                        return None
                    text = await resp.read()
                    return text
        except asyncio.TimeoutError:
            self.logger.error(f"rss: è¯·æ±‚ç«™ç‚¹ {url} è¶…æ—¶")
            return None
        except aiohttp.ClientError as e:
            self.logger.error(f"rss: è¯·æ±‚ç«™ç‚¹ {url} ç½‘ç»œé”™è¯¯: {str(e)}")
            return None
        except Exception as e:
            self.logger.error(f"rss: è¯·æ±‚ç«™ç‚¹ {url} å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}")
            return None

    async def cron_task_callback(self, url: str, user: str):
        """å®šæ—¶ä»»åŠ¡å›è°ƒ"""

        if url not in self.data_handler.data:
            return
        if user not in self.data_handler.data[url]["subscribers"]:
            return

        self.logger.info(f"RSS å®šæ—¶ä»»åŠ¡è§¦å‘: {url} - {user}")
        last_update = self.data_handler.data[url]["subscribers"][user]["last_update"]
        latest_link = self.data_handler.data[url]["subscribers"][user]["latest_link"]
        max_items_per_poll = self.max_items_per_poll
        # æ‹‰å– RSS
        rss_items = await self.poll_rss(
            url,
            num=max_items_per_poll,
            after_timestamp=last_update,
            after_link=latest_link,
        )
        max_ts = last_update

        # åˆ†è§£MessageSesion
        platform_name,message_type,session_id = user.split(":")

        # åˆ†å¹³å°å¤„ç†æ¶ˆæ¯
        if platform_name == "aiocqhttp" and self.is_compose:
            nodes = []
            for item in rss_items:
                comps = await self._get_chain_components(item)
                node = Comp.Node(
                            uin=0,
                            name="Astrbot",
                            content=comps
                        )
                nodes.append(node)
                self.data_handler.data[url]["subscribers"][user]["last_update"] = int(
                    time.time()
                )
                max_ts = max(max_ts, item.pubDate_timestamp)

            # åˆå¹¶æ¶ˆæ¯å‘é€
            if len(nodes) > 0:
                msc = MessageChain(
                    chain=nodes,
                    use_t2i_= self.t2i
                )
                await self.context.send_message(user, msc)
        else:
            # æ¯ä¸ªæ¶ˆæ¯å•ç‹¬å‘é€
            for item in rss_items:
                comps = await self._get_chain_components(item)
                msc = MessageChain(
                chain=comps,
                use_t2i_= self.t2i
            )
                await self.context.send_message(user, msc)
                self.data_handler.data[url]["subscribers"][user]["last_update"] = int(
                    time.time()
                )
                max_ts = max(max_ts, item.pubDate_timestamp)

        # æ›´æ–°æœ€åæ›´æ–°æ—¶é—´
        if rss_items:
            self.data_handler.data[url]["subscribers"][user]["last_update"] = max_ts
            self.data_handler.data[url]["subscribers"][user]["latest_link"] = rss_items[
                0
            ].link
            self.data_handler.save_data()
            self.logger.info(f"RSS å®šæ—¶ä»»åŠ¡ {url} æ¨é€æˆåŠŸ - {user}")
        else:
            self.logger.info(f"RSS å®šæ—¶ä»»åŠ¡ {url} æ— æ¶ˆæ¯æ›´æ–° - {user}")


    async def poll_rss(
        self,
        url: str,
        num: int = -1,
        after_timestamp: int = 0,
        after_link: str = "",
    ) -> List[RSSItem]:
        """ä»ç«™ç‚¹æ‹‰å–RSSä¿¡æ¯"""
        text = await self.parse_channel_info(url)
        if text is None:
            self.logger.error(f"rss: æ— æ³•è§£æç«™ç‚¹ {url} çš„RSSä¿¡æ¯")
            return []
        
        try:
            root = etree.fromstring(text)
        except Exception as e:
            self.logger.error(f"rss: XMLè§£æå¤±è´¥ {url}: {str(e)}")
            return []
        
        # æ£€æµ‹æ˜¯RSSè¿˜æ˜¯Atom
        is_atom = root.tag.endswith('feed') or 'atom' in root.tag.lower()
        
        # æ ¹æ®æ ¼å¼é€‰æ‹©itemè·¯å¾„ - ä½¿ç”¨local-name()é¿å…å‘½åç©ºé—´é—®é¢˜
        if is_atom:
            items = root.xpath("//*[local-name()='entry']")
        else:
            items = root.xpath("//item")

        cnt = 0
        rss_items = []

        for item in items:
            try:
                chan_title = (
                    self.data_handler.data[url]["info"]["title"]
                    if url in self.data_handler.data
                    else "æœªçŸ¥é¢‘é“"
                )

                # æå–æ ‡é¢˜
                if is_atom:
                    title_elem = item.xpath("*[local-name()='title']")
                else:
                    title_elem = item.xpath("title")
                
                title = title_elem[0].text if title_elem and title_elem[0].text else "æ— æ ‡é¢˜"
                if len(title) > self.title_max_length:
                    title = title[: self.title_max_length] + "..."

                # æå–é“¾æ¥
                if is_atom:
                    link_elem = item.xpath("*[local-name()='link']/@href")
                    link = link_elem[0] if link_elem else ""
                else:
                    link_elem = item.xpath("link")
                    link = link_elem[0].text if link_elem and link_elem[0].text else ""
                
                if link and not re.match(r"^https?://", link):
                    link = self.data_handler.get_root_url(url) + link

                # æå–æè¿°/å†…å®¹ - ä¼˜å…ˆä½¿ç”¨å®Œæ•´å†…å®¹
                content = ""
                description = ""
                summary = ""
                
                if is_atom:
                    # Atomæ ¼å¼ - ä½¿ç”¨local-name()
                    content_elem = item.xpath("*[local-name()='content']")
                    summary_elem = item.xpath("*[local-name()='summary']")
                    
                    if content_elem and content_elem[0].text:
                        content = content_elem[0].text
                    if summary_elem and summary_elem[0].text:
                        summary = summary_elem[0].text
                    description = content or summary
                else:
                    # RSSæ ¼å¼
                    desc_elem = item.xpath("description")
                    # å°è¯•è·å–content:encoded(æ›´å®Œæ•´çš„å†…å®¹)
                    content_elem = item.xpath("*[local-name()='encoded']")
                    
                    if content_elem and content_elem[0].text:
                        content = content_elem[0].text
                    if desc_elem and desc_elem[0].text:
                        description = desc_elem[0].text

                # æå–ä½œè€…
                author = ""
                if is_atom:
                    author_elem = item.xpath("*[local-name()='author']/*[local-name()='name']")
                    if author_elem and author_elem[0].text:
                        author = author_elem[0].text
                else:
                    author_elem = item.xpath("author") or item.xpath("*[local-name()='creator']")
                    if author_elem and author_elem[0].text:
                        author = author_elem[0].text
                
                # æå–åˆ†ç±»
                categories = []
                if is_atom:
                    cat_elems = item.xpath("*[local-name()='category']/@term")
                    categories = list(cat_elems)
                else:
                    cat_elems = item.xpath("category")
                    categories = [cat.text for cat in cat_elems if cat.text]
                
                # æå–é™„ä»¶(enclosure)
                enclosure_url = ""
                enclosure_type = ""
                enclosure_elem = item.xpath("enclosure")
                if enclosure_elem:
                    enclosure_url = enclosure_elem[0].get("url", "")
                    enclosure_type = enclosure_elem[0].get("type", "")
                
                # æå–è¯„è®ºé“¾æ¥
                comments_url = ""
                comments_elem = item.xpath("comments")
                if comments_elem and comments_elem[0].text:
                    comments_url = comments_elem[0].text
                
                # æå–GUID
                guid = ""
                if is_atom:
                    guid_elem = item.xpath("*[local-name()='id']")
                    if guid_elem and guid_elem[0].text:
                        guid = guid_elem[0].text
                else:
                    guid_elem = item.xpath("guid")
                    if guid_elem and guid_elem[0].text:
                        guid = guid_elem[0].text
                
                # å¤„ç†å†…å®¹ - ä½¿ç”¨å®Œæ•´å†…å®¹æˆ–æè¿°
                full_content = content or description
                pic_url_list = self.data_handler.strip_html_pic(full_content)
                
                # æ¸…ç†HTMLå¾—åˆ°çº¯æ–‡æœ¬æè¿°
                clean_description = self.data_handler.strip_html(description or content)
                clean_description = self.data_handler.smart_truncate(clean_description, self.description_max_length)
                
                # ä¿ç•™å®Œæ•´å†…å®¹
                clean_content = self.data_handler.strip_html(content) if content else ""

                # æå–æ—¥æœŸ
                pub_date = ""
                pub_date_timestamp = 0
                
                if is_atom:
                    date_elem = item.xpath("*[local-name()='updated']") or \
                               item.xpath("*[local-name()='published']")
                    if date_elem and date_elem[0].text:
                        pub_date = date_elem[0].text
                else:
                    date_elem = item.xpath("pubDate")
                    if date_elem and date_elem[0].text:
                        pub_date = date_elem[0].text
                
                # è§£ææ—¥æœŸ
                if pub_date:
                    pub_date_timestamp = self._parse_date(pub_date)
                
                # åˆ¤æ–­æ˜¯å¦ä¸ºæ–°å†…å®¹
                is_new = False
                if pub_date_timestamp > 0:
                    is_new = pub_date_timestamp > after_timestamp
                else:
                    is_new = link != after_link
                
                if is_new:
                    rss_items.append(
                        RSSItem(
                            chan_title=chan_title,
                            title=title,
                            link=link,
                            description=clean_description,
                            pubDate=pub_date,
                            pubDate_timestamp=pub_date_timestamp,
                            pic_urls=pic_url_list,
                            author=author,
                            categories=categories,
                            content=clean_content,
                            summary=summary,
                            enclosure_url=enclosure_url,
                            enclosure_type=enclosure_type,
                            comments_url=comments_url,
                            guid=guid
                        )
                    )
                    cnt += 1
                    if num != -1 and cnt >= num:
                        break
                elif pub_date_timestamp > 0:  # æœ‰æ—¥æœŸä½†ä¸æ˜¯æ–°å†…å®¹,åœæ­¢
                    break

            except Exception as e:
                self.logger.error(f"rss: è§£æRssæ¡ç›® {url} å¤±è´¥: {str(e)}")
                break

        return rss_items

    def _parse_date(self, date_str: str) -> int:
        """è§£æå„ç§æ—¥æœŸæ ¼å¼ä¸ºæ—¶é—´æˆ³"""
        if not date_str:
            return 0
        
        # å¸¸è§æ—¥æœŸæ ¼å¼
        date_formats = [
            "%a, %d %b %Y %H:%M:%S %z",      # RSSæ ‡å‡†: Wed, 02 Oct 2002 13:00:00 GMT
            "%a, %d %b %Y %H:%M:%S GMT",     # RSS GMTæ ¼å¼
            "%Y-%m-%dT%H:%M:%S%z",           # ISO 8601: 2002-10-02T13:00:00+00:00
            "%Y-%m-%dT%H:%M:%SZ",            # ISO 8601 UTC: 2002-10-02T13:00:00Z
            "%Y-%m-%dT%H:%M:%S.%f%z",        # ISO 8601å¸¦æ¯«ç§’
            "%Y-%m-%dT%H:%M:%S.%fZ",         # ISO 8601 UTCå¸¦æ¯«ç§’
            "%Y-%m-%d %H:%M:%S",             # ç®€å•æ ¼å¼
            "%Y/%m/%d %H:%M:%S",             # æ–œæ åˆ†éš”
        ]
        
        # é¢„å¤„ç†
        date_str = date_str.strip()
        if "GMT" in date_str:
            date_str = date_str.replace("GMT", "+0000")
        
        # å°è¯•å„ç§æ ¼å¼
        for fmt in date_formats:
            try:
                parsed = time.strptime(date_str, fmt)
                return int(time.mktime(parsed))
            except ValueError:
                continue
        
        # å¦‚æœéƒ½å¤±è´¥,è¿”å›å½“å‰æ—¶é—´
        self.logger.warning(f"æ— æ³•è§£ææ—¥æœŸæ ¼å¼: {date_str}")
        return int(time.time())
    
    def parse_rss_url(self, url: str) -> str:
        """è§£æRSS URLï¼Œç¡®ä¿ä»¥httpæˆ–httpså¼€å¤´"""
        if not re.match(r"^https?://", url):
            if not url.startswith("/"):
                url = "/" + url
            url = "https://" + url
        return url

    def _fresh_asyncIOScheduler(self):
        """åˆ·æ–°å®šæ—¶ä»»åŠ¡ï¼Œä½¿ç”¨å›ºå®šIDé˜²æ­¢ä»»åŠ¡å †ç§¯"""
        self.logger.info("åˆ·æ–°å®šæ—¶ä»»åŠ¡")
        
        # 1. æ”¶é›†å½“å‰é…ç½®ä¸­æ‰€æœ‰åº”è¯¥å­˜åœ¨çš„ä»»åŠ¡ ID
        active_job_ids = set()
        
        for url, info in self.data_handler.data.items():
            if url in ["rsshub_endpoints", "settings"]:
                continue
            
            for user, sub_info in info["subscribers"].items():
                # æ„é€ å”¯ä¸€ IDï¼šURL + User
                job_id = f"{url}|{user}"
                active_job_ids.add(job_id)
                
                try:
                    # æ·»åŠ æˆ–æ›´æ–°ä»»åŠ¡
                    # id: æŒ‡å®šå›ºå®šID
                    # replace_existing: å¦‚æœä»»åŠ¡å·²å­˜åœ¨ï¼Œåˆ™æ›´æ–°è§¦å‘å‚æ•°
                    self.scheduler.add_job(
                        self.cron_task_callback,
                        "cron",
                        **self.parse_cron_expr(sub_info["cron_expr"]),
                        args=[url, user],
                        id=job_id,
                        replace_existing=True
                    )
                except Exception as e:
                    self.logger.error(f"æ·»åŠ å®šæ—¶ä»»åŠ¡å¤±è´¥ {job_id}: {str(e)}")

        # 2. æ¸…ç†å·²ç»ä¸å†é…ç½®ä¸­çš„åºŸå¼ƒä»»åŠ¡
        # è·å–è°ƒåº¦å™¨ä¸­å½“å‰æ‰€æœ‰çš„ä»»åŠ¡
        current_jobs = self.scheduler.get_jobs()
        for job in current_jobs:
            # å¦‚æœè°ƒåº¦å™¨é‡Œçš„ä»»åŠ¡IDä¸åœ¨æˆ‘ä»¬éœ€è¦æ´»è·ƒçš„åˆ—è¡¨ä¸­ï¼Œè¯´æ˜è¯¥è®¢é˜…å·²è¢«åˆ é™¤
            if job.id not in active_job_ids:
                try:
                    self.scheduler.remove_job(job.id)
                    self.logger.info(f"æ¸…ç†åºŸå¼ƒä»»åŠ¡: {job.id}")
                except Exception as e:
                    self.logger.error(f"æ¸…ç†åºŸå¼ƒä»»åŠ¡å¤±è´¥ {job.id}: {str(e)}")

        self.logger.info(f"å®šæ—¶ä»»åŠ¡åˆ·æ–°å®Œæˆï¼Œå½“å‰è¿è¡Œä»»åŠ¡æ•°: {len(self.scheduler.get_jobs())}")

    async def _add_url(self, url: str, cron_expr: str, message: AstrMessageEvent):
        """å†…éƒ¨æ–¹æ³•:æ·»åŠ URLè®¢é˜…çš„å…±ç”¨é€»è¾‘"""
        user = message.unified_msg_origin
        if url in self.data_handler.data:
            latest_item = await self.poll_rss(url)
            if not latest_item:
                return message.plain_result(f"æ— æ³•è·å–RSSå†…å®¹,è¯·æ£€æŸ¥URLæ˜¯å¦æ­£ç¡®")
            self.data_handler.data[url]["subscribers"][user] = {
                "cron_expr": cron_expr,
                "last_update": latest_item[0].pubDate_timestamp,
                "latest_link": latest_item[0].link,
            }
        else:
            try:
                text = await self.parse_channel_info(url)
                if text is None:
                    return message.plain_result(f"æ— æ³•è®¿é—®è¯¥RSSæº,è¯·æ£€æŸ¥URLæ˜¯å¦æ­£ç¡®")
                title, desc = self.data_handler.parse_channel_text_info(text)
                latest_item = await self.poll_rss(url)
                if not latest_item:
                    return message.plain_result(f"RSSæºæ— å¯ç”¨å†…å®¹,è¯·æ£€æŸ¥URLæ˜¯å¦æ­£ç¡®")
            except Exception as e:
                return message.plain_result(f"è§£æé¢‘é“ä¿¡æ¯å¤±è´¥: {str(e)}")

            self.data_handler.data[url] = {
                "subscribers": {
                    user: {
                        "cron_expr": cron_expr,
                        "last_update": latest_item[0].pubDate_timestamp,
                        "latest_link": latest_item[0].link,
                    }
                },
                "info": {
                    "title": title,
                    "description": desc,
                },
            }
        self.data_handler.save_data()
        return self.data_handler.data[url]["info"]

    async def _get_chain_components(self, item: RSSItem):
        """ç»„è£…æ¶ˆæ¯é“¾"""
        comps = []
        # æ”¶é›†æ‰€æœ‰çš„æ–‡æœ¬è¡Œ
        text_lines = []
        
        # æ ‡é¢˜å’Œé¢‘é“ä¿¡æ¯
        text_lines.append(f"ğŸ“° {item.chan_title}")
        text_lines.append("â”€" * 30)
        text_lines.append(f"ğŸ“Œ {item.title}")
        
        # æ·»åŠ ä½œè€…å’Œåˆ†ç±»
        meta_info = []
        if item.author:
            meta_info.append(f"ğŸ‘¤ {item.author}")
        if item.categories:
              # æœ€å¤šæ˜¾ç¤º3ä¸ªåˆ†ç±»
            meta_info.append(f"ğŸ·ï¸ {', '.join(item.categories[:3])}")
        if item.pubDate and item.pubDate_timestamp > 0:
            # æ ¼å¼åŒ–æ—¥æœŸæ˜¾ç¤º
            import datetime
            dt = datetime.datetime.fromtimestamp(item.pubDate_timestamp)
            meta_info.append(f"ğŸ•’ {dt.strftime('%Y-%m-%d %H:%M')}")
        
        if meta_info:
            text_lines.append(" | ".join(meta_info))
        
        text_lines.append("â”€" * 30)
        
        # å†…å®¹ - ä½¿ç”¨å®Œæ•´å†…å®¹æˆ–æè¿°
        content_text = item.get_display_content(self.description_max_length)
        if content_text:
            # ç¡®ä¿å†…å®¹æœ¬èº«å‰åä¸å¸¦å¤šä½™ç©ºè¡Œ
            text_lines.append(content_text.strip())
        
        # é“¾æ¥
        if not self.is_hide_url and item.link:
            # æ·»åŠ ä¸€ä¸ªç©ºè¡Œåšåˆ†éš”
            text_lines.append("") 
            text_lines.append(f"ğŸ”— {item.link}")
        
        # é™„ä»¶ä¿¡æ¯(éŸ³é¢‘/è§†é¢‘)
        if item.enclosure_url:
            text_lines.append("") # ç©ºè¡Œåˆ†éš”
            enclosure_info = "ğŸ“ é™„ä»¶: "
            if "audio" in item.enclosure_type:
                enclosure_info += "ğŸµ éŸ³é¢‘ - "
            elif "video" in item.enclosure_type:
                enclosure_info += "ğŸ¬ è§†é¢‘ - "
            else:
                enclosure_info += "ğŸ“„ æ–‡ä»¶ - "
            enclosure_info += item.enclosure_url
            text_lines.append(enclosure_info)
            
        # è¯„è®ºé“¾æ¥
        if item.comments_url:
            text_lines.append(f"ğŸ’¬ è¯„è®º: {item.comments_url}")

        # å›¾ç‰‡æ ‡é¢˜
        has_images = self.is_read_pic and item.pic_urls
        if has_images:
            text_lines.append("") # ç©ºè¡Œåˆ†éš”
            text_lines.append(f"ğŸ“· å›¾ç‰‡ ({len(item.pic_urls)}å¼ ):")

        # ç”Ÿæˆæ–‡æœ¬
        final_text = "\n".join(text_lines)
        comps.append(Comp.Plain(final_text))

        # å¤„ç†å›¾ç‰‡ç»„ä»¶
        if has_images:
            # å¦‚æœmax_pic_itemä¸º-1åˆ™ä¸é™åˆ¶å›¾ç‰‡æ•°é‡
            temp_max_pic_item = len(item.pic_urls) if self.max_pic_item == -1 else self.max_pic_item
            
            for idx, pic_url in enumerate(item.pic_urls[:temp_max_pic_item], 1):
                # è·å–æœ¬åœ°è·¯å¾„
                file_path = await self.pic_handler.get_image_file(pic_url)
                
                if file_path:
                    # ä½¿ç”¨ fromFileSystem å‘é€æœ¬åœ°æ–‡ä»¶
                    comps.append(Comp.Image.fromFileSystem(file_path))
                else:
                    # å›¾ç‰‡åŠ è½½å¤±è´¥çš„ä¿¡æ¯
                    comps.append(Comp.Plain(f"\n[âŒ] å›¾{idx} åŠ è½½å¤±è´¥\n"))
            
            # å¦‚æœè¿˜æœ‰æ›´å¤šå›¾ç‰‡æœªæ˜¾ç¤º
            if len(item.pic_urls) > temp_max_pic_item:
                count = len(item.pic_urls) - temp_max_pic_item
                comps.append(Comp.Plain(f"\n... è¿˜æœ‰ {count} å¼ å›¾ç‰‡æœªæ˜¾ç¤º"))
        
        return comps


    def _is_url_or_ip(self,text: str) -> bool:
        """
        åˆ¤æ–­ä¸€ä¸ªå­—ç¬¦ä¸²æ˜¯å¦ä¸ºç½‘å€ï¼ˆhttp/https å¼€å¤´ï¼‰æˆ– IP åœ°å€ã€‚
        """
        url_pattern = r"^(?:http|https)://.+$"
        ip_pattern = r"^((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$"
        return bool(re.match(url_pattern, text) or re.match(ip_pattern, text))

    @filter.command_group("rss", alias={"RSS"})
    def rss(self):
        """RSSè®¢é˜…æ’ä»¶

        å¯ä»¥è®¢é˜…å’Œç®¡ç†å¤šä¸ªRSSæºï¼Œæ”¯æŒcronè¡¨è¾¾å¼è®¾ç½®æ›´æ–°é¢‘ç‡

        cron è¡¨è¾¾å¼æ ¼å¼ï¼š
        * * * * *ï¼Œåˆ†åˆ«è¡¨ç¤ºåˆ†é’Ÿ å°æ—¶ æ—¥ æœˆ æ˜ŸæœŸï¼Œ* è¡¨ç¤ºä»»æ„å€¼ï¼Œæ”¯æŒèŒƒå›´å’Œé€—å·åˆ†éš”ã€‚ä¾‹ï¼š
        1. 0 0 * * * è¡¨ç¤ºæ¯å¤© 0 ç‚¹è§¦å‘ã€‚
        2. 0/5 * * * * è¡¨ç¤ºæ¯ 5 åˆ†é’Ÿè§¦å‘ã€‚
        3. 0 9-18 * * * è¡¨ç¤ºæ¯å¤© 9 ç‚¹åˆ° 18 ç‚¹è§¦å‘ã€‚
        4. 0 0 1,15 * * è¡¨ç¤ºæ¯æœˆ 1 å·å’Œ 15 å· 0 ç‚¹è§¦å‘ã€‚
        æ˜ŸæœŸçš„å–å€¼èŒƒå›´æ˜¯ 0-6ï¼Œ0 è¡¨ç¤ºæ˜ŸæœŸå¤©ã€‚
        """
        pass

    @rss.group("rsshub")
    def rsshub(self, event: AstrMessageEvent):
        """RSSHubç›¸å…³æ“ä½œ

        å¯ä»¥æ·»åŠ ã€æŸ¥çœ‹ã€åˆ é™¤RSSHubçš„ç«¯ç‚¹
        """
        pass

    @rsshub.command("add")
    async def rsshub_add(self, event: AstrMessageEvent, url: str):
        """æ·»åŠ ä¸€ä¸ªRSSHubç«¯ç‚¹

        Args:
            url: RSSHubæœåŠ¡å™¨åœ°å€ï¼Œä¾‹å¦‚ï¼šhttps://rsshub.app
        """
        if url.endswith("/"):
            url = url[:-1]
        # æ£€æŸ¥æ˜¯å¦ä¸ºurlæˆ–ip
        if not self._is_url_or_ip(url):
            yield event.plain_result("è¯·è¾“å…¥æ­£ç¡®çš„URL")
            return
        # æ£€æŸ¥è¯¥ç½‘å€æ˜¯å¦å·²å­˜åœ¨
        elif url in self.data_handler.data["rsshub_endpoints"]:
            yield event.plain_result("è¯¥RSSHubç«¯ç‚¹å·²å­˜åœ¨")
            return
        else:
            self.data_handler.data["rsshub_endpoints"].append(url)
            self.data_handler.save_data()
            yield event.plain_result("æ·»åŠ æˆåŠŸ")

    @rsshub.command("list")
    async def rsshub_list(self, event: AstrMessageEvent):
        """åˆ—å‡ºæ‰€æœ‰å·²æ·»åŠ çš„RSSHubç«¯ç‚¹"""
        ret = "å½“å‰Botæ·»åŠ çš„rsshub endpointï¼š\n"
        yield event.plain_result(
            ret
            + "\n".join(
                [
                    f"{i}: {x}"
                    for i, x in enumerate(self.data_handler.data["rsshub_endpoints"])
                ]
            )
        )

    @rsshub.command("remove")
    async def rsshub_remove(self, event: AstrMessageEvent, idx: int):
        """åˆ é™¤ä¸€ä¸ªRSSHubç«¯ç‚¹

        Args:
            idx: è¦åˆ é™¤çš„ç«¯ç‚¹ç´¢å¼•ï¼Œå¯é€šè¿‡listå‘½ä»¤æŸ¥çœ‹
        """
        if idx < 0 or idx >= len(self.data_handler.data["rsshub_endpoints"]):
            yield event.plain_result("ç´¢å¼•è¶Šç•Œ")
            return
        else:
            # TODO:åˆ é™¤å¯¹åº”çš„å®šæ—¶ä»»åŠ¡
            self.scheduler.remove_job()
            self.data_handler.data["rsshub_endpoints"].pop(idx)
            self.data_handler.save_data()
            yield event.plain_result("åˆ é™¤æˆåŠŸ")

    @rss.command("add")
    async def add_command(
        self,
        event: AstrMessageEvent,
        idx: int,
        route: str,
        minute: str,
        hour: str,
        day: str,
        month: str,
        day_of_week: str,
    ):
        """é€šè¿‡RSSHubè·¯ç”±æ·»åŠ è®¢é˜…

        Args:
            idx: RSSHubç«¯ç‚¹ç´¢å¼•ï¼Œå¯é€šè¿‡/rss rsshub listæŸ¥çœ‹
            route: RSSHubè·¯ç”±ï¼Œéœ€ä»¥/å¼€å¤´
            minute: Cronè¡¨è¾¾å¼åˆ†é’Ÿå­—æ®µ
            hour: Cronè¡¨è¾¾å¼å°æ—¶å­—æ®µ
            day: Cronè¡¨è¾¾å¼æ—¥æœŸå­—æ®µ
            month: Cronè¡¨è¾¾å¼æœˆä»½å­—æ®µ
            day_of_week: Cronè¡¨è¾¾å¼æ˜ŸæœŸå­—æ®µ
        """
        if idx < 0 or idx >= len(self.data_handler.data["rsshub_endpoints"]):
            yield event.plain_result(
                "ç´¢å¼•è¶Šç•Œ, è¯·ä½¿ç”¨ /rss rsshub list æŸ¥çœ‹å·²ç»æ·»åŠ çš„ rsshub endpoint"
            )
            return
        if not route.startswith("/"):
            yield event.plain_result("è·¯ç”±å¿…é¡»ä»¥ / å¼€å¤´")
            return

        url = self.data_handler.data["rsshub_endpoints"][idx] + route
        cron_expr = f"{minute} {hour} {day} {month} {day_of_week}"

        ret = await self._add_url(url, cron_expr, event)
        if isinstance(ret, MessageEventResult):
            yield ret
            return
        else:
            chan_title = ret["title"]
            chan_desc = ret["description"]

        # åˆ·æ–°å®šæ—¶ä»»åŠ¡
        self._fresh_asyncIOScheduler()

        yield event.plain_result(
            f"æ·»åŠ æˆåŠŸã€‚é¢‘é“ä¿¡æ¯ï¼š\næ ‡é¢˜: {chan_title}\næè¿°: {chan_desc}"
        )

    @rss.command("add-url")
    async def add_url_command(
        self,
        event: AstrMessageEvent,
        url: str,
        minute: str,
        hour: str,
        day: str,
        month: str,
        day_of_week: str,
    ):
        """ç›´æ¥é€šè¿‡Feed URLæ·»åŠ è®¢é˜…

        Args:
            url: RSS Feedçš„å®Œæ•´URL
            minute: Cronè¡¨è¾¾å¼åˆ†é’Ÿå­—æ®µ
            hour: Cronè¡¨è¾¾å¼å°æ—¶å­—æ®µ
            day: Cronè¡¨è¾¾å¼æ—¥æœŸå­—æ®µ
            month: Cronè¡¨è¾¾å¼æœˆä»½å­—æ®µ
            day_of_week: Cronè¡¨è¾¾å¼æ˜ŸæœŸå­—æ®µ
        """
        cron_expr = f"{minute} {hour} {day} {month} {day_of_week}"
        ret = await self._add_url(url, cron_expr, event)
        if isinstance(ret, MessageEventResult):
            yield ret
            return
        else:
            chan_title = ret["title"]
            chan_desc = ret["description"]

        # åˆ·æ–°å®šæ—¶ä»»åŠ¡
        self._fresh_asyncIOScheduler()

        yield event.plain_result(
            f"æ·»åŠ æˆåŠŸã€‚é¢‘é“ä¿¡æ¯ï¼š\næ ‡é¢˜: {chan_title}\næè¿°: {chan_desc}"
        )

    @rss.command("list")
    async def list_command(self, event: AstrMessageEvent):
        """åˆ—å‡ºå½“å‰æ‰€æœ‰è®¢é˜…çš„RSSé¢‘é“"""
        user = event.unified_msg_origin
        ret = "å½“å‰è®¢é˜…çš„é¢‘é“ï¼š\n"
        subs_urls = self.data_handler.get_subs_channel_url(user)
        cnt = 0
        for url in subs_urls:
            info = self.data_handler.data[url]["info"]
            ret += f"{cnt}. {info['title']} - {info['description']}\n"
            cnt += 1
        yield event.plain_result(ret)

    @rss.command("remove")
    async def remove_command(self, event: AstrMessageEvent, idx: int):
        """åˆ é™¤ä¸€ä¸ªRSSè®¢é˜…

        Args:
            idx: è¦åˆ é™¤çš„è®¢é˜…ç´¢å¼•ï¼Œå¯é€šè¿‡/rss listæŸ¥çœ‹
        """
        subs_urls = self.data_handler.get_subs_channel_url(event.unified_msg_origin)
        if idx < 0 or idx >= len(subs_urls):
            yield event.plain_result("ç´¢å¼•è¶Šç•Œ, è¯·ä½¿ç”¨ /rss list æŸ¥çœ‹å·²ç»æ·»åŠ çš„è®¢é˜…")
            return
        url = subs_urls[idx]
        self.data_handler.data[url]["subscribers"].pop(event.unified_msg_origin)

        self.data_handler.save_data()

        # åˆ·æ–°å®šæ—¶ä»»åŠ¡
        self._fresh_asyncIOScheduler()
        yield event.plain_result("åˆ é™¤æˆåŠŸ")

    @rss.command("get")
    async def get_command(self, event: AstrMessageEvent, idx: int):
        """è·å–æŒ‡å®šè®¢é˜…çš„æœ€æ–°å†…å®¹

        Args:
            idx: è¦æŸ¥çœ‹çš„è®¢é˜…ç´¢å¼•ï¼Œå¯é€šè¿‡/rss listæŸ¥çœ‹
        """
        subs_urls = self.data_handler.get_subs_channel_url(event.unified_msg_origin)
        if idx < 0 or idx >= len(subs_urls):
            yield event.plain_result("ç´¢å¼•è¶Šç•Œ, è¯·ä½¿ç”¨ /rss list æŸ¥çœ‹å·²ç»æ·»åŠ çš„è®¢é˜…")
            return
        url = subs_urls[idx]
        rss_items = await self.poll_rss(url)
        if not rss_items:
            yield event.plain_result("æ²¡æœ‰æ–°çš„è®¢é˜…å†…å®¹")
            return
        item = rss_items[0]
        # åˆ†è§£MessageSesion
        platform_name,message_type,session_id = event.unified_msg_origin.split(":")
        # æ„é€ è¿”å›æ¶ˆæ¯é“¾
        comps = await self._get_chain_components(item)
        # åŒºåˆ†å¹³å°
        if(platform_name == "aiocqhttp" and self.is_compose):
            node = Comp.Node(
                    uin=0,
                    name="Astrbot",
                    content=comps
                )
            yield event.chain_result([node]).use_t2i(self.t2i)
        else:
            yield event.chain_result(comps).use_t2i(self.t2i)
